{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "69be7653",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4534f30",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers datasets peft bitsandbytes accelerate pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2123e12",
      "metadata": {},
      "source": [
        "## Step 2: Mount Google Drive (to save model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1174e731",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ… Google Drive mounted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6837fcf4",
      "metadata": {},
      "source": [
        "## Step 3: Upload Training Data\n",
        "\n",
        "Run this cell, then:\n",
        "1. Click \"Choose Files\"\n",
        "2. Select `ent_ai_triage/modelling/data/training_data.jsonl` from your project (the urgency-only JSONL, ~6.5k examples)\n",
        "3. Upload (a few seconds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7ead17",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"ðŸ“¤ Upload training_data.jsonl from your Mac...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify\n",
        "import os\n",
        "for filename in uploaded:\n",
        "    size = os.path.getsize(filename)\n",
        "    print(f\"âœ… Uploaded: {filename} ({size/1024/1024:.1f}MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2497219c",
      "metadata": {},
      "source": [
        "## Step 4: Load & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd18199d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load JSONL\n",
        "data = []\n",
        "with open('training_data.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "print(f\"âœ… Loaded {len(data)} training examples\")\n",
        "\n",
        "# Format as instruction-following (must match training_data.jsonl: instruction / input / output)\n",
        "def format_prompt(example):\n",
        "    instruction = example[\"instruction\"]\n",
        "    input_text = example[\"input\"]\n",
        "    output_text = example[\"output\"]  # one of: routine, semi-urgent, urgent\n",
        "    prompt = f\"\"\"Instruction: {instruction}\n",
        "\n",
        "Input: {input_text}\n",
        "\n",
        "Output: {output_text}\"\"\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "dataset = dataset.map(format_prompt, remove_columns=list(dataset.column_names))\n",
        "\n",
        "# Split 90/10\n",
        "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_data = split[\"train\"]\n",
        "eval_data = split[\"test\"]\n",
        "\n",
        "print(f\"ðŸ“Š Train: {len(train_data)}, Eval: {len(eval_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "192ee7a2",
      "metadata": {},
      "source": [
        "## Step 5: Load Model with QLoRA (4-bit quantization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5467cb1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model_name = \"Qwen/Qwen2-0.5B\"\n",
        "\n",
        "print(f\"ðŸ¤– Loading {model_name} with 4-bit quantization...\")\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Prepare for QLoRA training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(\"âœ… Model loaded and prepared for QLoRA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68536c5",
      "metadata": {},
      "source": [
        "## Step 6: Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905be8a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(f\"âœ… LoRA applied - trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe42e3dd",
      "metadata": {},
      "source": [
        "## Step 7: Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23962314",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=1024,  # long dialogue transcripts need more than 512\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "print(\"ðŸ”„ Tokenizing data...\")\n",
        "train_data = train_data.map(tokenize, batched=True, num_proc=2)\n",
        "eval_data = eval_data.map(tokenize, batched=True, num_proc=2)\n",
        "print(\"âœ… Tokenization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d3d00d",
      "metadata": {},
      "source": [
        "## Step 8: Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9f46d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/finetuned-ent-llm\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    logging_steps=20,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3e9c79",
      "metadata": {},
      "source": [
        "## Step 9: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eda9de7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapter into base model (so you get a full model for Ollama conversion later)\n",
        "print(\"Merging LoRA weights into base model...\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save to Google Drive (full merged model)\n",
        "output_dir = \"/content/drive/My Drive/finetuned-ent-llm\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"âœ… Model saved to Google Drive: {output_dir}\")\n",
        "print(\"\\nðŸ“¥ Next: download folder to your Mac, then run from project root:\")\n",
        "print(\"   python ent_ai_triage/modelling/code/export_to_ollama.py --model-dir <path-to-downloaded-folder> --ollama-model-name ent-triage-qwen2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c7d173",
      "metadata": {},
      "source": [
        "## Step 10: Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb76684",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=200\n",
        ")\n",
        "\n",
        "# Test prompt (must match the instruction format from training_data.jsonl)\n",
        "test_input = \"\"\"Instruction: You are an ENT triage expert. Classify the urgency of this patient as routine, semi-urgent, or urgent based on their symptoms.\n",
        "\n",
        "Input: Patient reports severe throat pain and difficulty swallowing for 3 days.\n",
        "\n",
        "Output:\"\"\"\n",
        "\n",
        "result = pipe(test_input, max_new_tokens=10, do_sample=False)\n",
        "print(\"ðŸ§ª Test inference:\")\n",
        "print(result[0][\"generated_text\"])\n",
        "print(\"\\n(Expected: Output: urgent or semi-urgent)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8754ae8e",
      "metadata": {},
      "source": [
        "## Step 11: After Colab â€” Push to Ollama (on your Mac)\n",
        "\n",
        "1. **Download** the `finetuned-ent-llm` folder from Google Drive to your Mac.\n",
        "2. **Place** it at `ent_ai_triage/modelling/model/finetuned-ent-llm/` (or any path you prefer).\n",
        "3. **Convert to GGUF** (Ollama uses GGUF). From your project root:\n",
        "   - Install [llama.cpp](https://github.com/ggerganov/llama.cpp) or use: `ollama run qwen2:0.5b` to confirm Ollama is installed.\n",
        "   - Convert HuggingFace â†’ GGUF (e.g. `python convert-hf-to-gguf.py <model_dir>` from llama.cpp, or use [ollama/import](https://github.com/ollama/ollama/blob/main/docs/import.md) if supported).\n",
        "4. **Create Ollama model**: `ollama create ent-triage-qwen2 -f Modelfile` (from the folder that has the Modelfile; generate it with `export_to_ollama.py` first).\n",
        "5. **Update** `ent_ai_triage/app/config.py`: set `OLLAMA_MODEL_NAME = \"ent-triage-qwen2\"`, then restart the API.\n",
        "\n",
        "For detailed steps see `ent_ai_triage/modelling/FINETUNING_GUIDE.md`."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
